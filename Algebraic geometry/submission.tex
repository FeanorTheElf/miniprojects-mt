\documentclass{scrartcl}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[style=english]{csquotes}
\usepackage[language=english, backend=biber, style=alphabetic, sorting=nyt]{biblatex}

\addbibresource{bibliography.bib}

\title{Miniproject - Algebraic Geometry}
\author{Simon Pohmann}
\date{}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Proj}{\mathbb{P}}
\newcommand{\Aff}{\mathbb{A}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Gr}{\mathrm{Gr}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\contradiction}{\text{contradiction}}
\newcommand{\extpow}{\mathchoice{{\textstyle\bigwedge}}
    {{\bigwedge}}
    {{\textstyle\wedge}}
    {{\scriptstyle\wedge}}}
\newcommand{\vspan}{\mathrm{span}}
\newcommand{\divides}{\ | \ }
\newcommand\restr[2]{{
    \left.\kern-\nulldelimiterspace
    #1
    \vphantom{\big|}
    \right|_{#2}
}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{example}[definition]{Example}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{theorem}[definition]{Theorem}

\begin{document}
\maketitle

\section{Part I}

\begin{definition}
    Let $V$ be a vector space.
    Then define the $d$-th exterior power as
    \begin{equation*}
        \extpow^d(V) := V^{\otimes d} \ / \ \sum_{i = 1}^{d - 1} V^{\otimes (i - 1)} \otimes \vspan\bigl\{ v \otimes v' + v' \otimes v \ \bigm| \ v, v' \in V \bigr\} \otimes V^{\otimes (d - i - 1)}
    \end{equation*}
    Use the notation $v_1 \wedge ... \wedge v_d := [v_1 \otimes ... \otimes v_d] \in \extpow^k(V)$.
\end{definition}

\begin{lemma}
    \label{prop:basic_properties_exterior_product}
    Let $v_1, ..., v_d \in V$. Have for $\pi \in S_d$ that
    \begin{equation*}
        v_{\pi(1)} \wedge ... \wedge v_{\pi(k)} = \sgn(\pi) (v_1 \wedge ... \wedge v_d)
    \end{equation*}
    Furthermore if $v_i = v_j$ for some $i \neq j$, then
    \begin{equation*}
        v_1 \wedge ... \wedge v_d = 0
    \end{equation*}
\end{lemma}
\begin{proof}
    Note that
    \begin{equation*}
        u \wedge v \wedge v' \wedge w = -(u \wedge v' \wedge v \wedge w)
    \end{equation*}
    for all $u \in \extpow^{i - 1}(V), w \in \extpow^{(d - i - 1)}(V), v, v' \in V$.

    Every $\pi \in S_d$ has a decomposition $\pi = \xi_1 ... \xi_n$ into transpositions $\xi_i$.
    Applying this inductively, we find that
    \begin{equation*}
        v_1 \wedge ... \wedge v_d = \sgn(\xi_i ... \xi_n) (v_{(\xi_i ... \xi_n)(1)} \wedge ... v_{(\xi_i ... \xi_n)(k)})
    \end{equation*}
    and so
    \begin{equation*}
        v_1 \wedge ... \wedge v_d = \sgn(\pi) (v_{\pi(1)} \wedge ... \wedge v_{\pi(k)})
    \end{equation*}

    Furthermore, we find that
    \begin{equation*}
        u \wedge v \wedge v \wedge w = -(u \wedge v \wedge v \wedge w) = 0
    \end{equation*}
    must be zero.
    Hence, if $v_1, ..., v_d \in V$ with $v_i = v_j$ for some $i \neq j$, then there is a permutation $\pi \in S_d$ with $\pi(1) = i, \pi(2) = j$ and
    \begin{equation*}
        v_1 \wedge ... \wedge v_d = (\sgn(\pi))(v_i \wedge v_j \wedge v_{\pi(3)} \wedge ... \wedge v_{\pi(k)}) = \sgn(\pi) 0 = 0
    \end{equation*}
\end{proof}

\begin{lemma}[1a]
    Let $\dim(V) \leq 3$. Then every element of $\extpow^k(V)$ is decomposable.
\end{lemma}
\begin{proof}
    Now let $v_1, v_2, v_3$ be a set of generators of $V$.
    Consider $u_1 = \sum \lambda_i v_i, u_2 = \sum_i \mu_i v_i, u_3 = \sum_i \rho_i v_i$.
    Then by applying Lemma~\ref{prop:basic_properties_exterior_product}, we see that 
    \begin{align*}
        u_1 \wedge u_2 =& \sum_{i, j} \ \lambda_i \mu_j (\underbrace{v_i \wedge v_j}_{\mathclap{\text{$= 0$ if $i = j$}}}) = \sum_{i < j} \lambda_i \mu_j (v_i \wedge v_j) - \sum_{i > j} \lambda_i \mu_j (v_i \wedge v_j) \\
        =& \sum_{i < j} (\lambda_i \mu_j - \lambda_j \mu_i)(v_i \wedge v_j) = \alpha (v_1 \wedge v_2) + \beta (v_1 \wedge v_3) + \gamma (v_2 \wedge v_3) \\
        =& \begin{cases}
            \beta v_1 + \gamma v_2 \wedge \frac \alpha \beta v_2 + v_3 & \text{if $\beta \neq 0$} \\
            \alpha v_1 - \gamma v_3 \wedge v_2 & \text{otherwise}
        \end{cases}
    \end{align*}
    and
    \begin{align*}
        u_1 \wedge u_2 \wedge u_3 =& \sum_{i, j, l} \ \lambda_i \mu_j \rho_l (\underbrace{v_i \wedge v_j \wedge v_l}_{\mathclap{\text{$= 0$ unless $i, j, l$ pairwise distinct}}}) \\
        =& \sum_{\pi \in S_3} \lambda_{\pi(1)} \mu_{\pi(2)} \rho_{\pi(3)} (v_{\pi(1)} \wedge v_{\pi(2)} \wedge v_{\pi(3)}) \\
        =& \sum_{\pi \in S_3} \lambda_{\pi(1)} \mu_{\pi(2)} \rho_{\pi(3)} \sgn(\pi) (v_1 \wedge v_2 \wedge v_3) \\
        =& (v_1 \wedge v_2 \wedge v_3) \sum_{\pi \in S_3} \lambda_{\pi(1)} \mu_{\pi(2)} \rho_{\pi(3)} \sgn(\pi)
    \end{align*}
    are decomposable.
    Further, it is easy to see from Lemma~\ref{prop:basic_properties_exterior_product} that $\extpow^k(V) = \{ 0 \}$ for $k \geq 4$, which is trivially decomposable.
\end{proof}

\begin{example}[1b]
    Consider $V = k^4$. 
    Then the element $w := (e_1 \wedge e_2) + (e_3 \wedge e_4) \in \extpow^2(V)$ is not decomposable. 
\end{example}
\begin{proof}
    Assume it was, then there are $a, b \in k^4$ such that
    \begin{equation*}
        w = \sum_i a_i e_i \wedge \sum_j b_j e_j = \sum_{i < j} (a_i b_j - a_j b_i) (e_i \wedge e_j)
    \end{equation*}
    In other words
    \begin{equation*}
        a_1b_2 - a_2b_1 = 1, \ a_3b_4 - a_4b_3 = 1, \ a_i b_j - a_j b_i = 0 \ \text{for all $(i, j) \neq (1, 2), (3, 4)$}
    \end{equation*}
    Clearly $a_1 b_2 \neq 0$ or $a_2 b_1 \neq 0$.
    Similarly, have $a_3 b_4 \neq 0$ or $a_4 b_3 \neq 0$.
    As all expressions are symmetric w.r.t swapping $a_1, b_2$ with $a_2, b_1$ and $a_3, b_4$ with $a_4, b_3$, we may assume wlog that $a_1 b_2, a_3 b_4 \neq 0$.

    Have $a_1 b_4 = a_4 b_1$ and $a_2 b_4 = a_4 b_2$.
    We know that $a_1 b_4 \neq 0$ and so
    \begin{equation*}
        \frac {a_2} {a_1} = \frac {a_2 b_4} {a_1 b_4} = \frac {a_4 b_2} {a_4 b_1} = \frac {b_2} {b_1} \ \Rightarrow \ a_2 b_1 = a_1 b_2
    \end{equation*}
    This contradicts $a_1 b_2 - a_2 b_1 = 1$.
\end{proof}

\begin{lemma}
    \label{prop:linear_transform_extpow}
    Let $A = (a_{ij}) \in \GL_d(k)$ and $v_1, ..., v_d \in V$.
    Then
    \begin{equation*}
        \Bigl( \sum_j a_{1j}v_j \Bigl) \wedge ... \wedge \Bigl( \sum_{j} a_{dj} v_j \Bigr) = \det(A) (v_1 \wedge ... \wedge v_d)
    \end{equation*}
\end{lemma}
\begin{proof}
    By a direct computation using Lemma~\ref{prop:basic_properties_exterior_product}, we find
    \begin{align*}
        &\Bigl( \sum_j a_{ij} v_j \Bigr) \wedge ... \wedge \Bigl( \sum_j a_{dj} v_j \Bigr) = \sum_{j_1, ..., j_d} a_{1j_1} ... a_{dj_d} (v_{j_1} \wedge ... \wedge v_{j_d}) \\
        = &\sum_{\pi \in S_d} a_{1\pi(1)} ... a_{d\pi(d)} (v_{\pi(1)} \wedge ... \wedge v_{\pi(d)}) \\
        = &\sum_{\pi \in S_d} a_{1\pi(1)} ... a_{d\pi(d)} \sgn(\pi) (v_1 \wedge ... \wedge v_d) \\
        = &(v_1 \wedge ... \wedge v_d) \ \sum_{\pi \in S_d} \sgn(\pi) \prod_{j = 1}^d a_{j\pi(j)} = \det(A) (v_1 \wedge ... \wedge v_d)
    \end{align*}
    where the last equality holds due to the Leibniz determinant formula.
\end{proof}
\begin{lemma}
    \label{prop:extpow_zero_iff_independent}
    For $v_1, ..., v_d \in V$ have
    \begin{equation*}
        v_1 \wedge ... \wedge v_d = 0 \ \Leftrightarrow \ v_1, ..., v_d \ \text{linearly dependent}
    \end{equation*}
\end{lemma}
\begin{proof}
    For the direction $\Leftarrow$, assume that $v_1, ..., v_d$ are not independent. 
    Then there is a nonzero vector $a_1 \in k^d$ with $\sum a_{1i} v_i = 0$.
    Clearly, we can extend $a_1$ to a basis $a_1, ..., a_d$ of $k^d$, which gives a matrix $A = (a_{ij}) \in \GL_d(k)$.

    However by Lemma~\ref{prop:linear_transform_extpow} we now get
    \begin{align*}
        0 =& 0 \wedge \Bigl( \sum_j a_{2j} v_j \Bigr) \wedge ... \wedge \Bigl( \sum_j a_{dj} v_j \Bigr) = \Bigl( \sum_j a_{1j} v_j \Bigr) \wedge ... \wedge \Bigl( \sum_j a_{dj} v_j \Bigr) \\
        =& \det(A) (v_1 \wedge ... \wedge v_d)
    \end{align*}
    and so $v = v_1 \wedge ... \wedge v_d = 0$ as $\det(A) \neq 0$.

    Direction $\Rightarrow$ TODO
\end{proof}
\begin{lemma}
    \label{prop:subspace_closed}
    Let $v \in V$ and $u \in \extpow^{d - 1} U$ for a linear subspace $U \leq V$. 
    If $v \wedge u \in \extpow^d U$ then $v \in U$ or $u = 0$.
\end{lemma}
\begin{proof}
    TODO
\end{proof}

\begin{lemma}[1c]
    \label{prop:characterization_decomposable_d_even}
    Let $d$ be even.
    An element $\omega \in \extpow^d V$ is decomposable if and only if $\omega \wedge \omega \in \extpow^{2d} V$ is zero.
\end{lemma}
\begin{proof}
    The direction $\Rightarrow$ even holds generally. Assume $\omega = v_1 \wedge ... \wedge v_d$.
    Then
    \begin{equation*}
        \omega \wedge \omega = v_1 \wedge ... \wedge v_d \wedge v_1 \wedge ... \wedge v_d = 0
    \end{equation*}
    by Lemma~\ref{prop:basic_properties_exterior_product}. 
    The other direction is more interesting.

    Let $\omega = v_1 + ... + v_t$ for linearly independent decomposable vectors $v_i \in \extpow^2 V$.
    Then
    \begin{align*}
        0 =& \omega \wedge \omega = \sum_{i, j} v_i \wedge v_j = \sum_{i < j} (v_i \wedge v_j) + (v_j \wedge v_i) \\
        =& \sum_{i < j} 2(v_i \wedge v_j) = 2\sum_i v_i \wedge \Bigl( \sum_{j > i} v_j \Bigr)
    \end{align*}
    Here we used that the permutation $\bigl(1 \ 2d\bigr)\bigl(2 \ (2d - 1)\bigr)...\bigl(d \ (d + 1)\bigr) \in S_{2d}$ has always sign $1$ (since $d$ is even).

    Note that for any nonzero decomposable vector
    \begin{equation*}
        u_1 \wedge u_2 \in \Bigl( \extpow^2 \vspan\{v_2, ..., v_t\} \Bigr) \setminus \{0\}
    \end{equation*}
    find
    \begin{equation*}
        u_1, u_2 \in \vspan\{v_2, ..., v_t\}
    \end{equation*}
    In particular, we know that
    \begin{equation*}
        v_1 \wedge \Bigl( \sum_{j > i} v_j \Bigr) \in \extpow^2 \vspan\{v_2, ..., v_t\}
    \end{equation*}
    and so $v_1 \in \vspan\{v_2, ..., v_t\}$ unless $\sum_{j > i} v_j = 0$ by Lemma~\ref{prop:subspace_closed}.
    We assumed that the $v_i$ are linearly independent, so the former would give a contradiction.
    Hence $\sum_{j > i} v_j = 0$ and thus $t = 1$, i.e. $\omega = v_1$ is decomposable.
\end{proof}

\section{Part II}

In this part, we want to consider the connection of external powers to the Grassmanian.
First of all, assume there are two bases $v_1, ..., v_d$ and $u_1, ..., u_d$ of a $d$-dimensional vector space $U$. 
Then there exists a basis change matrix $A = (a_{ij}) \in \GL_d(k)$ with
\begin{equation*}
    u_i = \sum_j a_{ij} v_j
\end{equation*}
So by Lemma~\ref{prop:linear_transform_extpow}, it follows that
\begin{equation*}
    u_1 \wedge ... \wedge u_d = \det(A) (v_1 \wedge ... \wedge v_d)
\end{equation*}
As $v_1, ..., v_d$ resp. $u_1, ..., u_d$ are bases, they are linearly independent and in particular, we see that
\begin{equation*}
    v_1 \wedge ... \wedge v_d \neq 0 \quad \text{and} \quad u_1 \wedge ... \wedge u_d \neq 0
\end{equation*}
by Lemma~\ref{prop:extpow_zero_iff_independent}.
Hence they have well-defined images $[v_1 \wedge ... \wedge v_d]$ resp. $[u_1 \wedge ... \wedge u_d]$ in the projective space $\Proj(\extpow^d V)$.
By the above, find
\begin{equation*}
    [v_1 \wedge ... \wedge v_d] = [u_1 \wedge ... \wedge u_d]
\end{equation*}
This allows us to study the Grassmanian $\Gr(d, V)$ of a fixed vector space $V$.
\begin{definition}
    Define the map
    \begin{equation*}
        \phi: \Gr(d, V) \to \Proj(\extpow^d V), \quad \vspan\{v_1, ..., v_d\} \mapsto [v_1 \wedge ... \wedge v_d]
    \end{equation*}
    which is well-defined by Lemma~\ref{prop:linear_transform_extpow} as described above.
\end{definition}
\begin{lemma}[1a]
    We have
    \begin{equation*}
        \mathrm{im}\phi = D := \{ [v] \in \Proj(\extpow^d V) \ | \ \text{$v$ decomposable}\}
    \end{equation*}
\end{lemma}
\begin{proof}
    First of all, note that the set $D$ is well-defined, as $v$ is decomposable if and only if $\lambda v$ is decomposable, for all $\lambda \in k^*$.

    By definition of $\phi$, we can directly observe that $\mathrm{im}\phi \subseteq D$.
    So consider an element $[v] \in D$.
    As $v$ is decomposable, it follows that $v = v_1 \wedge ... \wedge v_d$ for $v_i \in V$.
    Not it suffices to show that the $v_i$ are linearly independent, then clearly $\vspan\{v_1, ..., v_d\}$ is a well-defined $d$-dimensional vector subspace of $V$, thus an element of $\Gr(d, V)$.

    This follows directly from Lemma~\ref{prop:extpow_zero_iff_independent}.
\end{proof}
\begin{definition}
    Let $\Gr(d, n) := \Gr(d, k^n)$.
\end{definition}
In the lecture, we considered an embedding of $\Gr(d, n)$ into projective space given by minors of the basis matrix.
This corresponds to the following definition.
\begin{definition}
    Define the map
    \begin{align*}
        \rho: \Gr(d, n) &\to \Proj\Bigl(k^{\{1, ..., n\}^{(d)}}\Bigr) \cong \Proj^{{n \choose d} - 1}, \\
        \vspan\{v_1, ..., v_d\} &\mapsto \Bigl[ \det\left(\begin{matrix}
            v_{1i_1} & ... & v_{di_1} \\
            \vdots & \ddots & \vdots \\
            v_{1i_d} & ... & v_{di_d}
        \end{matrix}\right) \Bigr]_{\{i_1, ..., i_d\} \in \{1, ..., n\}^{(d)}}
    \end{align*}
    where $\{1, ..., n\}^{(d)} := \{ I \subset \{1, ..., n\} \ | \ \#I = d \}$ is the set of all $d$-element subsets of $\{1, ..., n\}$.
\end{definition}
\begin{lemma}
    \label{prop:isomorphism_extpow_det}
    There is a linear isomorphism
    \begin{align*}
        f: \extpow^d k^n &\to k^{\{1, ..., n\}^{(d)}}, \\
        \sum_j v_1^{(j)} \wedge ... \wedge v_d^{(j)} &\mapsto \Bigl( \sum_j \det\left(\begin{matrix}
            v_{1i_1}^{(j)} & ... & v_{di_1}^{(j)} \\
            \vdots & \ddots & \vdots \\
            v_{1i_d}^{(j)} & ... & v_{di_d}^{(j)}
        \end{matrix}\right) \Bigr)_{\{i_1, ..., i_d\} \in \{1, ..., n\}^{(d)}}
    \end{align*}
\end{lemma}
\begin{proof}
    For vectors $v_1, ..., v_d$ and $I = \{i_1, ..., i_d\} \in \{1, ..., n\}^{(d)}$ write
    \begin{equation*}
        A_I(v_1, ..., v_d) := \left(\begin{matrix*}
            v_{1i_1} & ... & v_{di_1} \\
            \vdots & \ddots & \vdots \\
            v_{1i_d} & ... & v_{di_d}
        \end{matrix*}\right)
    \end{equation*}
    First of all, we show that $f$ is well-defined.
    Note that the tensor product can be described as
    \begin{align*}
        V^{\otimes d} := k^{V \times ...\times V} / \vspan\{ &\bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes \bigl(v_i + v'_i\bigl) \otimes v_{i + 1} \otimes ... \otimes v_d\bigl) \\
        &- \bigl(v_1 \otimes ... \otimes v_d\bigl) - \bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes v'_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl), \\
        & \bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes \lambda v_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl) \\
        &- \lambda\bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes v_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl) \ | \ i \leq d, v_j, v'_i \in V\}
    \end{align*}
    where $v_1 \otimes ... \otimes v_d := \chi_{(v_1, ..., v_d)}$.
    Hence the external power can be described as
    \begin{align*}
        \extpow^d V := k^{V \times ... \times V} / \vspan\Bigl\{ &\bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes \bigl(v_i + v'_i\bigl) \otimes v_{i + 1} \otimes ... \otimes v_d\bigl) \\
        &- \bigl(v_1 \otimes ... \otimes v_d\bigl) - \bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes v'_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl), \\
        & \bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes \lambda v_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl) \\
        &- \lambda\bigl(v_1 \otimes ... \otimes v_{i - 1} \otimes v_i \otimes v_{i + 1} \otimes ... \otimes v_d\bigl), \\
        &v_1 \otimes ... v_{j - 1} \otimes (v_j \otimes v_{j + 1} + v_{j + 1} \otimes v_j) \otimes v_{j + 1} \otimes ... \otimes v_d \\
        &\quad | \ i \leq d, \ j \leq d - 1, \ v_1, ..., v_d, v'_i \in V\Bigr\}
    \end{align*}
    So it suffices to show that for all $I \in \{1, ..., n\}^{(d)}$ and vectors $v_1, ..., v_d, v'_i \in V$
    \begin{align*}
        \det(A_I(v_1, ..., v_i + v'_i, ..., v_d)) = \det(A_I(v_1, ..., v_d)) + \det(A_I(v_1, ..., v'_i, ..., v_d))
    \end{align*}
    and
    \begin{align*}
        \det(A_I(v_1, ..., \lambda v_i, ..., v_d)) = \lambda\det(A_I(v_1, ..., v_d))
    \end{align*}
    and
    \begin{equation*}
        \det(A_I(v_1, ..., v_{j + 1}, v_j, ..., v_d)) = -\det(A_I(v_1, ..., v_d))
    \end{equation*}
    However, these properties follow from the well-known properties of the determinant.
    In particular, $\det$ is linear in each column and swapping columns negates the determinant.
    It follows that $f$ is indeed well-defined.

    It is clear by definition that $f$ is linear, so it is left to show that it is bijective. 
    To show surjectivity, note that the $\pm e_I, I \in \{1, ..., n\}^{(d)}$ form a basis of $k^{\{1, ..., n\}^{(d)}}$.
    Clearly for $I = \{i_1, ..., i_d\}, J \in \{1, ..., n\}^{(d)}$ we have that
    \begin{equation*}
        f(e_{i_1} \wedge ... \wedge e_{i_d})_J = \det(A_J(e_{i_1}, ..., e_{i_j})) = \begin{cases}
            0 & \text{if $J \not\subseteq I$} \\
            \pm 1 & \text{if $J \subseteq I$}
        \end{cases}
    \end{equation*}
    so $f(e_{i_1} \wedge ... \wedge e_{i_d}) = e_I$ and we deduce that $\mathrm{im}f = \Proj^{\{1, ..., k\}^{(d)}}$.

    Finally, note that
    \begin{equation*}
        e_{i_1} \wedge ... \wedge e_{i_d}
    \end{equation*}
    for $i_1 < ... < i_d$ form a basis of $\extpow^d k^n$.
    Clearly, they span $\extpow^d k^n$, and the following argument shows that they are linearly independent.
    Assume
    \begin{equation*}
        \sum_{i_1 < ... < i_d} \lambda_{i_1, ..., i_d} (e_{i_1} \wedge ... \wedge e_{i_d}) = 0
    \end{equation*}
    Then
    \begin{equation*}
        0 = e_1 \wedge \Bigl( \sum_{1 < i_2 < ... < i_d} \lambda_{1, i_2, ..., i_d} (e_{i_2} \wedge .. \wedge e_{i_d}) \Bigr) + \sum_{1 < i_1 < ... < i_d} \lambda_{i_1, ..., i_d} (e_{i_1} \wedge ... \wedge e_{i_d} )
    \end{equation*}
    Clearly $e_1 \notin \vspan\{e_2, ..., e_n\}$ and so by Lemma~\ref{prop:subspace_closed} we see that
    \begin{equation*}
        \sum_{1 < i_2 < ... < i_d} \lambda_{1, i_2, ..., i_d} (e_{i_2} \wedge ... \wedge e_{i_d}) = 0
    \end{equation*}
    Repeating this argument inductively shows that $\lambda_{1, 2, ..., d} = 0$.
    As $k^n$ is symmetric w.r.t. permuting the $e_j$, we see that all $\lambda_{i_1, ..., i_d} = 0$ are zero.

    It follows that $\dim(\extpow^d k^n) = \dim(\Proj^{\{1, ..., n\}^{(d)}})$ and we find that $f$ is also injective.
\end{proof}
\begin{corollary}[2b]
    \label{prop:image_phi}
    Let $\bar{f}: \Proj(\extpow^d k^n) \to \Proj^{{n \choose d} - 1}$ be the map $f$ from before modulo $k^*$.
    Then
    \begin{equation*}
        \rho = \bar{f} \circ \phi
    \end{equation*}
    and in particular, we see that $\phi(Gr(d, n))$ is a projective variety and isomorphic to $\rho(\Gr(d, n))$.
\end{corollary}
\begin{proposition}[2c]
    The map $\phi$ is injective.
\end{proposition}
\begin{proof}
    Consider two $d$-dimensional subspaces $U, W$ of $k^n$ with $\phi(U) = \phi(W)$.
    Let $u_1, ..., u_l$ be a basis of $U \cap W$ and extend it to bases $u_1, ..., u_d$ of $U$ and $u_1, ..., u_l, w_{l + 1}, ..., w_d$ of $W$.
    As $\phi(U) = \phi(W)$, we can assume that the $u_i, w_i$ are scaled such that
    \begin{align*}
        0 =& (u_1 \wedge ... \wedge u_d) - (u_1 \wedge ... \wedge u_l \wedge w_{l + 1} \wedge ... \wedge w_d) \\
        =& u_1 \wedge ... \wedge u_l \wedge \bigl( (u_{l + 1} \wedge ... \wedge u_d) - (w_{l + 1} \wedge ... \wedge w_d) \bigr)
    \end{align*}
    By Lemma~\ref{prop:subspace_closed} we see that
    \begin{equation*}
        u_2 \wedge ... \wedge u_l \wedge \bigl( (u_{l + 1} \wedge ... \wedge u_d) - (w_{l + 1} \wedge ... \wedge w_d) \bigr) = 0
    \end{equation*}
    as $u_1 \notin \vspan\{u_2, ..., u_d, w_{l + 1}, ..., w_d\}$.
    Inductively, this argument shows that
    \begin{equation*}
        (u_{l + 1} \wedge ... \wedge u_d) - (w_{l + 1} \wedge ... \wedge w_d) = 0
    \end{equation*}
    If $l < d$, we can now apply Lemma~\ref{prop:subspace_closed} again to see that
    \begin{equation*}
        u_{l + 1} \in \vspan\{ u_{l + 2}, ..., u_d, w_{l + 1}, ..., w_d \}
    \end{equation*}
    as $u_{l + 2} \wedge ... \wedge u_d \neq 0$ by Lemma~\ref{prop:extpow_zero_iff_independent}.
    However, this contradicts the linear independence of $u_{l + 1}, ..., u_d, w_{l + 1}, ..., w_d$.
    Hence it must be $l = d$ and so $U = W$.
\end{proof}

\section{Part III}
In this part, we want to investigate the geometric properties of the Grassmanian resp. the image of $\phi$.
First of all, we introduce coordinates on $\Proj(\extpow^d k^n)$.
\begin{definition}
    Note that in the proof of Lemma~\ref{prop:isomorphism_extpow_det} it was shown that $v_{i_1} \wedge ... \wedge v_{i_d}$ for $i_1 < ... < i_d$ is a basis of $\extpow^d k^n$ if $v_1, ..., v_n$ is a basis of $V$.
    We introduce the homogeneous coordinates w.r.t. that basis, namely
    \begin{align*}
        x: \Proj(\extpow^d k^n) &\to \Proj_k^{\{1, ..., n\}^{(d)}} \cong \Proj_k^{{n \choose d} - 1}, \\
        \Bigl[ \sum_{i_1 < ... < i_d} \lambda_{i_1, ..., i_d} (v_{i_1} \wedge ... \wedge v_{i_d}) \Bigr] &\mapsto \bigl[ \lambda_{i_1, ..., i_d} \bigr]_{i_1 < ... < i_d}
    \end{align*}
    The individual coordinates will be denoted by $x_I$ for some $I \in \{1, ..., n\}^{(d)}$ or $x_{i_1, ..., i_d}$ for $i_1 < ... < i_d$.
\end{definition}
\begin{proposition}[3a]
    \label{prop:equations_grassmanian_2}
    For the embedding $\phi: \Gr(2, V) \to \extpow^2 V$ we have
    \begin{equation*}
        \Gr(2, V) \cong \mathrm{im}\phi = \V(I)
    \end{equation*}
    where
    \begin{equation*}
        I := \langle x_{i, j}x_{u, v} + x_{i, v}x_{j, u} - x_{i, u}x_{j, v} \ | \ i < j < u < v \rangle \leq k\bigl[\Proj(\extpow^d V)\bigr] = k[x_{i, j} \ | \ i < j]
    \end{equation*}
\end{proposition}
\begin{proof}
    By Lemma~\ref{prop:image_phi} we have that
    \begin{equation*}
        [\omega] \in \mathrm{im}\phi \ \Leftrightarrow \ \omega\ \text{decomposable}
    \end{equation*}
    and so by Lemma~\ref{prop:characterization_decomposable_d_even}
    \begin{equation*}
        \omega \in \mathrm{im}\phi \ \Leftrightarrow \ \omega \wedge \omega = 0
    \end{equation*}
    In $\Proj(\extpow^d V)$ we find that
    \begin{align*}
        &\Bigl( \sum_{i < j} x_{i, j} (e_i \wedge e_j) \Bigr) \wedge \Bigl( \sum_{u < v} x_{u, v} (e_u \wedge e_v) \Bigr) = \sum_{\substack{i < j\\u < v}} x_{i, j} x_{u, v} (e_i \wedge e_j \wedge e_u \wedge e_v) \\
        =& \ 2\sum_{\substack{i < j < u < v}} x_{i, j} x_{u, v} (e_i \wedge e_j \wedge e_u \wedge e_v) + 2\sum_{\substack{i < u < j < v}} x_{i, j} x_{u, v} (e_i \wedge e_j \wedge e_u \wedge e_v) \\
        &+ 2\sum_{\substack{u < i < j < v}} x_{i, j} x_{u, v} (e_i \wedge e_j \wedge e_u \wedge e_v) \\
        =&2 \sum_{i < j < u < v} (x_{i, j} x_{u, v} - x_{i, u} x_{j, v} + x_{j, u} x_{i, v}) (e_i \wedge e_j \wedge e_u \wedge e_v)
    \end{align*}
    As the $e_i \wedge e_j \wedge e_u \wedge e_v$ are linearly independent, we see that for $[\omega] \in \Proj(\extpow^2 V)$ we have
    \begin{equation*}
        [\omega] \in \mathrm{im}\phi \ \Leftrightarrow \ \forall i < j < u < v: \ (x_{i, j} x_{u, v} + x_{i, v} x_{j, u} - x_{i, u} x_{j, v})(\omega) = 0
    \end{equation*}
    Hence $\mathrm{im}\phi = \V(I)$.
\end{proof}
\begin{example}[2b]
    For $n = 4$, Prop.~\ref{prop:equations_grassmanian_2} yields that $\Gr(2, 4) \cong \mathrm{im}\phi = \V(I)$ where
    \begin{equation*}
        I = \langle x_{1, 2} x_{3, 4} + x_{1, 4} x_{2, 3} - x_{1, 3} x_{2, 4} \rangle \in k[x_{1, 2}, x_{1, 3}, x_{1, 4}, x_{2, 3}, x_{2, 4}, x_{3, 4}]
    \end{equation*}
    Changing the indices used for the coordinates, we find
    \begin{equation*}
        \Gr(2, 4) = \V(x_0 x_5 + x_2 x_3 - x_2 x_4)
    \end{equation*}
    which is exactly what we found in the lecture.
\end{example}
\begin{example}[2c]
    For $n = 5$, Prop.~\ref{prop:equations_grassmanian_2} yields that $\Gr(2, 5) \cong \mathrm{im}\phi = \V(I)$ where
    \begin{align*}
        I = \langle &x_{1, 2} x_{3, 4} + x_{1, 4} x_{2, 3} - x_{1, 3} x_{2, 4}, \quad x_{1, 2} x_{3, 5} + x_{1, 5} x_{2, 3} - x_{1, 3} x_{2, 5}, \\
        &x_{1, 2} x_{4, 5} + x_{1, 5} x_{2, 4} - x_{1, 4} x_{2, 5}, \quad x_{1, 3} x_{4, 5} + x_{1, 5} x_{3, 4} - x_{1, 4} x_{3, 5}, \\ 
        &x_{2, 3} x_{4, 5} + x_{2, 5} x_{3, 4} - x_{2, 4} x_{3, 5} \rangle \leq k\bigl[ \Proj(\extpow^2 k^5) \bigr]
    \end{align*}
    Using the following Sage-code, we can compute the number of intersection points of $\Gr(2, 5)$ with 3-dimensional hyperplanes, and find a probable value for its degree.
    \lstinputlisting[language = python]{./examples.sage}
    This shows that the degree of $\Gr(2, 5)$ is indeed 5, as expected from the degree formula mentioned in the lecture.
    \begin{equation*}
        \deg(\Gr(d, n)) = (d(n - d))! \frac {1! \cdot 2! \cdot ... \cdot (d - 1)!} {(n - d)! \cdot (n - d + 1)! \cdot ... \cdot (n - 1)!}
    \end{equation*}
    which yields
    \begin{equation*}
        \deg(\Gr(2, 5)) = 6! \frac {1!} {3! \cdot 4!} = \frac {6 \cdot 5} {3!} = 5
    \end{equation*}
\end{example}
To investigate the properties of $\phi(\Gr(2, n))$ for larger $n$, we use one tool I encountered during an earlier course on Computational Commutative Algebra and Algebraic Geometry.
\begin{proposition}[Macaulay Basis Theorem]
    Let $\preceq$ be a graded monomial ordering on $R = k[x_0, ..., x_n]$.
    Then for an ideal $I \leq R$ have that the monomials $x_0^{\alpha_0} ... x_n^{\alpha_n} \notin \mathrm{lt}(I)$ are a $k$-vector space basis of $R/I$.

    Here $\mathrm{lt}(I)$ is the leading term ideal of $I$, i.e. the ideal generated by the leading terms of all $f \in I$, w.r.t. $\preceq$.
\end{proposition}
\begin{proof}
    See \cite{kreuzer}.
\end{proof}
\begin{lemma}
    Define the graded reverse monomial ordering $\preceq$ on $R := k[x_{i, j} \ | \ i < j]$ where the variables $x_{i, j}$ are ordered co-lexicographically w.r.t. $(i, j)$, i.e.
    \begin{equation*}
        x_{i, j} \leq x_{u, v} \ :\Leftrightarrow \ (i, j) \leq_{\text{colex}} (u, v)
    \end{equation*}
    Moreover, let
    \begin{equation*}
        I := \langle x_{i, j}x_{u, v} + x_{i, v}x_{j, u} - x_{i, u}x_{j, v} \ | \ i < j < u < v \rangle \leq R
    \end{equation*}
    be the ideal defining $\phi(\Gr(d, V))$ that was considered above. Then
    \begin{equation*}
        \mathrm{lt}(I) = J := \langle x_{i, v} x_{j, u} \ | \ i < j < u < v \rangle \leq R
    \end{equation*}
\end{lemma}
\begin{proof}
    Note that for $i < j < u < v$ have $x_{u, v}, x_{j, v} \succ x_{i, v} \succ x_{i, j}, x_{i, u}, x_{j, u}$.
    Thus the leading term of $x_{i, j} x_{u, v} + x_{i, v} x_{j, u} - x_{i, u} x_{j, v}$ is
    \begin{equation*}
        \mathrm{lt}(x_{i, j} x_{u, v} + x_{i, v} x_{j, u} - x_{i, u} x_{j, v}) = x_{i, v} x_{j, u}
    \end{equation*}
    It follows that $J \subseteq \mathrm{lt}(I)$.

    For the other direction, we use a quite lengthy degree argument.
    Sadly, the only argument I came up with is extremely technical.
    We try to present it as clearly as possible, at the cost of only sketching some parts.
    In the end, I checked it using Computer Algebra, and everything fits together.

    Consider homogeneous polynomials $f_{i, j, u, v} \in R$ and
    \begin{equation*}
        F = \sum_{i < j < u < v} f_{i, j, u, v} \ (x_{i, v} x_{j, u} - x_{i, u} x_{j, v} + x_{i, j} x_{u, v})
    \end{equation*}
    We want to show that $\mathrm{lt}(F) \in J$.
    
    Let
    \begin{equation*}
        f_{i, j, u, v} = \sum_{\alpha \in \N^N} c^{(i, j, u, v)}_\alpha x^\alpha
    \end{equation*}
    Then
    \begin{equation*}
        F = \sum_{\alpha \in \N^N} \sum_{i < j < u < v} c^{(i, j, u, v)}_\alpha x^\alpha \ (x_{i, v} x_{j, u} - x_{i, u} x_{j, v} + x_{i, j} x_{u, v})
    \end{equation*}
    and so there exists $\alpha \in \N^N$ and $\epsilon \in k^*$ with
    \begin{equation*}
        \mathrm{lt}(F) = \epsilon \cdot \mathrm{lt}\Bigl( \sum_{i < j < u < v} c_\alpha^{(i, j, u, v)} x^\alpha \ (x_{i, v} x_{j, u} - x_{i, u} x_{j, v} + x_{i, j} x_{u, v}) \Bigr)
    \end{equation*}
    Hence, we may assume wlog that all the $f_{i, j, u, v}$ are scaled monomials.

    Now, observe that all monomials in $F$ are of the form
    \begin{align*}
        \epsilon \ x_{i, v} \ x_{j, u} \ f_{i, j, u, v} \quad \text{or} \quad 
        \epsilon \ x_{i, u} \ x_{j, v} \ f_{i, j, u, v} \quad \text{or} \quad 
        \epsilon \ x_{i, j} \ x_{u, v} \ f_{i, j, u, v} 
    \end{align*}
    where $\epsilon \in k^*$ and $i < j < u < v$.
    In particular, this is true for the leading term $\mathrm{lt}(F)$.
    In the first of those case, clearly $\mathrm{lt}(F) \in J$.

    So consider now the second case, i.e. $\mathrm{lt}(F) = \epsilon \ x_{i, u} \ x_{j, v} \ f_{i, j, u, v}$.
    Since $x_{i, v} \ x_{j, u} \ f_{i, j, u, v} \succ x_{i, u} \ x_{j, v} \ f_{i, j, u, v}$, we see that the term $x_{i, v} \ x_{j, u} \ f_{i, j, u, v}$ cannot occur in $F$, i.e. must ``cancel out''.
    Hence the monomial $\epsilon \ x_{i, v} \ x_{j, u} \ f_{i, j, u, v}$ has a nonzero coefficient in
    \begin{equation*}
        F - f_{i, j, u, v} x_{i, v} x_{j, u} = \sum_{\substack{a < b < c < d\\(a, b, c, d) \neq (i, j, u, v)}} f_{a, b, c, d} \ (x_{a, d} x_{b, c} - x_{a, c} x_{b, d} + x_{a, b} x_{c, d})
    \end{equation*}
    and so for $(a, b, c, d) \neq (i, j, u, v), \epsilon' \in k^*$ have that
    \begin{align*}
        f_{i, j, u, v} \ x_{i, v} \ x_{j, u} &= \epsilon' f_{a, b, c, d} \ x_{a, d} \ x_{b, c} \quad \text{or}\\
        f_{i, j, u, v} \ x_{i, v} \ x_{j, u} &= \epsilon' f_{a, b, c, d} \ x_{a, c} \ x_{b, d} \quad \text{or}\\
        f_{i, j, u, v} \ x_{i, v} \ x_{j, u} &= \epsilon' f_{a, b, c, d} \ x_{a, b} \ x_{c, d}
    \end{align*}
    However, the second and third case imply that $f_{a, b, c, d} \ x_{a, d} \ x_{b, c} \succ f_{i, j, u, v} \ x_{i, v} \ x_{j, u}$.
    Hence, the ``new'' monomial $f_{a, b, c, d} \ x_{a, d} \ x_{b, c}$ also has to ``cancel out'' in the sum representation of $F$, as comes after $\mathrm{lt}(F) = \epsilon x_{i, u} \ x_{j, v} \ f_{i, j, u, v}$ in the order $\preceq$.
    So applying the whole argument inductively (induction on the number of monomials $\succ f_{i, j, u, v} \ x_{i, v} \ x_{j, u}$ that occur in any of the polynomials we work with), we end up in the first case
    \footnote{There is a small argument missing here, namely that we apply the next argument on each step of the induction, to show the claim for $f_{i, j, u, v} \ x_{i, v} \ x_{j, u}$. However, it should be easy to see that this is possible.}.

    If $(a, d) = (i, v)$ then $(b, c) \neq (j, u)$ and $x_{b, c} \divides f_{i, j, u, v}$.
    Thus
    \begin{equation*}
        x_{b, c} \ x_{i, u} \ x_{j, v} \divides \mathrm{lt}(F) = \epsilon \ x_{i, u} \ x_{j, v} \ f_{i, j, u, v}
    \end{equation*}
    with $i < j < u < v$ and $i < b < c < v$. 
    No matter how $j, u, b, c$ are ordered relatively to each other, we see that in each possible case $x_{b, c} \ x_{i, u} \ x_{j, v} \in J$:
    \begin{align*}
        j < u < b < c \ &\Rightarrow \ x_{j, v} x_{b, c} \in J \\
        j < b < u < c \ &\Rightarrow \ x_{j, v} x_{b, c} \in J \\
        j < b < c < u \ &\Rightarrow \ x_{i, u} x_{b, c} \in J \\
        b < j < u < c \ &\Rightarrow \ f_{i, b, c, v} \ x_{i, c} \ x_{b, v} \succ f_{i, j, u, v} \ x_{i, u} \ x_{j, v}, \ \contradiction \\
        b < j < c < u \ &\Rightarrow \ x_{i, u} x_{b, c} \in J \\
        b < c < j < u \ &\Rightarrow \ x_{i, u} x_{b, c} \in J \\
    \end{align*}
    Hence $\mathrm{lt}(F) \in J$.

    It is now left to consider the case $(b, c) = (j, u)$ and the case $(a, d) \neq (i, v), (b, c) \neq (j, u)$.
    The former can be dealt with in exactly the same way, by noting that $x_{a, d} \divides f_{i, j, u, v}$.
    In the latter case, we even find $x_{a, d} \ x_{b, c} \divides f_{i, j, u, v}$ and a very similar argument works.

    Finally, one must also consider the third ``big'' case, namely that
    \begin{equation*}
        \mathrm{lt}(F) = \epsilon \ x_{i, j} \ x_{u, v} \ f_{i, j, u, v} 
    \end{equation*}
    Again, you can do this similarly as before, but now two monomials ``cancel out''.
    We will not present this here as well.
\end{proof}

\paragraph{Another idea} Let
\begin{equation*}
    m_n(d) := |\{ x^\alpha \ \text{monomial} \ | \ \deg(x^\alpha) = d, \ x_{iv}x_{ju} \ \not| \ x^\alpha \}|
\end{equation*}
and
\begin{equation*}
    s_n(d) := |\{ x^\alpha \ \text{multilinear monomial} \ | \ \deg(x^\alpha) = d, \ x_{iv}x_{ju} \ \not| \ x^\alpha \}|
\end{equation*}
Then for sufficiently large $d$ have
\begin{equation*}
    m_n(d) = \sum_{i = 0}^n s_n(i) \left(\!\!{i \choose d - i}\!\!\right) = \sum_{i = 0}^n s_n(i) {d - 1 \choose i - 1}
\end{equation*}
Note now that we are only interested in the leading term of the polynomial, which is (more or less) given by the largest $i \leq n$ such that $s_n(i) \neq 0$.
So in fact, it suffices to find this.

For example, the dimension of $\Gr(2, n)$ is the size of the largest independent set $- 1$ in this strange graph I have been considering.

For example, I claim:
\begin{theorem}
    \begin{equation*}
        \dim(\Gr(2, n)) = 2n - 4
    \end{equation*}
\end{theorem}
\printbibliography
\end{document}